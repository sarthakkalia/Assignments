{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c6c0f61",
   "metadata": {},
   "source": [
    "# Na√Øve bayes-1 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe30fa2-cd35-4de7-8a39-52878b6c6bea",
   "metadata": {},
   "source": [
    "#### Q1. What is Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897885c4-7d09-4dbf-9cd1-78e6ad0617d8",
   "metadata": {},
   "source": [
    "Bayes' theorem, named after the Reverend Thomas Bayes, is a fundamental theorem in probability theory that describes how to update the probability of a hypothesis (an event or proposition) in light of new evidence or information. It is a way to revise or update the probability of a hypothesis based on additional data or observations.\r\n",
    "\r\n",
    "The theorem can be mathematically expressed as:\r\n",
    "\r\n",
    "\\[ P(A \\mid B) = \\frac{P(B \\mid A) \\times P(A)}{P(B)} \\]\r\n",
    "\r\n",
    "Where:\r\n",
    "- \\( P(A \\mid B) \\) is the probability of event \\( A \\) given event \\( B \\) has occurred (this is the posterior probability).\r\n",
    "- \\( P(B \\mid A) \\) is the probability of event \\( B \\) given event \\( A \\) has occurred (this is the likelihood).\r\n",
    "- \\( P(A) \\) is the prior probability of event \\( A \\) before observing \\( B \\).\r\n",
    "- \\( P(B) \\) is the total probability of event \\( B \\) (also known as the marginal probability of \\( B \\)).\r\n",
    "\r\n",
    "In words, Bayes' theorem states that the probability of \\( A \\) given \\( B \\) is proportional to the probability of \\( B \\) given \\( A \\) multiplied by the prior probability of \\( A \\), and then divided by the total probability of \\( B \\).\r\n",
    "\r\n",
    "This theorem is widely used in statistics, machine learning, and various fields of science for tasks such as classification, prediction, and inference. It provides a formal way to update beliefs or hypotheses based on new evidence, making it a powerful tool for decision-making under uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482d2b61-25f6-4366-81ac-9b7d0a2b1897",
   "metadata": {},
   "source": [
    "#### Q2. What is the formula for Bayes' theorem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb46bab2-a75b-4cb6-8fd1-82aa03a45516",
   "metadata": {},
   "source": [
    "Bayes' theorem is a fundamental rule in probability theory that describes the probability of an event based on prior knowledge of conditions related to the event. The theorem is mathematically expressed as:\r\n",
    "\r\n",
    "\\[ P(A \\mid B) = \\frac{P(B \\mid A) \\times P(A)}{P(B)} \\]\r\n",
    "\r\n",
    "Here's what each component represents:\r\n",
    "\r\n",
    "- \\( P(A \\mid B) \\) is the probability of event \\( A \\) occurring given that event \\( B \\) has occurred. This is called the posterior probability of \\( A \\) given \\( B \\).\r\n",
    "- \\( P(B \\mid A) \\) is the probability of event \\( B \\) occurring given that event \\( A \\) has occurred. This is called the likelihood of \\( B \\) given \\( A \\).\r\n",
    "- \\( P(A) \\) is the probability of event \\( A \\) occurring. This is called the prior probability of \\( A \\) before observing \\( B \\).\r\n",
    "- \\( P(B) \\) is the probability of event \\( B \\) occurring. This is called the marginal probability of \\( B \\).\r\n",
    "\r\n",
    "In words, Bayes' theorem tells us how to update our belief in the probability of \\( A \\) given new evidence \\( B \\). We multiply the prior probability of \\( A \\) by the likelihood of \\( B \\) given \\( A \\), and then normalize by dividing by the total probability of \\( B \\).\r\n",
    "\r\n",
    "This theorem is widely used in various fields such as statistics, machine learning, and data science for tasks involving inference, prediction, and decision-making under uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b95c03-2561-414b-98c9-d3e72de3dafe",
   "metadata": {},
   "source": [
    "#### Q3. How is Bayes' theorem used in practice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f5cd5a-42c4-48bf-a49a-bc5f2446be18",
   "metadata": {},
   "source": [
    "Bayes' theorem is a powerful tool used in various practical applications, particularly in statistics, machine learning, and data science. Here are some common ways Bayes' theorem is applied in practice:\r\n",
    "\r\n",
    "1. **Bayesian Inference**: Bayes' theorem forms the basis of Bayesian inference, a statistical approach for updating beliefs or probabilities about a hypothesis as new evidence becomes available. It allows us to incorporate prior knowledge or beliefs (prior probability) and update these beliefs based on observed data (likelihood) to obtain a posterior probability distribution.\r\n",
    "\r\n",
    "2. **Medical Diagnosis**: Bayes' theorem is used in medical diagnosis to update the probability of a disease given certain symptoms. The prior probability of having a disease is updated based on the likelihood of observing those symptoms given the disease (sensitivity and specificity of tests), resulting in a more accurate posterior probability of the disease.\r\n",
    "\r\n",
    "3. **Spam Filtering**: In email spam filtering, Bayes' theorem is employed to classify emails as spam or not spam. The theorem helps update the probability that an email is spam given certain words or features observed in the email (likelihood), using prior probabilities derived from a training dataset.\r\n",
    "\r\n",
    "4. **Machine Learning**: Bayesian methods are used in machine learning for probabilistic modeling and inference. Bayesian classifiers, such as Naive Bayes classifiers, use Bayes' theorem to calculate the probability of a class given input features, making them efficient and effective for tasks like text classification and sentiment ain5the data.\r\n",
    "\r\n",
    "7. **Natural Language Processing**: Bayes' theorem is used in various NLP tasks, such as language modeling, machine translation, and speech recognition. It helps estimate the probability of a sequence of words given a particular context, enabling more accurate and context-awarning algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e349383-c15f-49b2-9547-31d213bc62b7",
   "metadata": {},
   "source": [
    "#### Q4. What is the relationship between Bayes' theorem and conditional probability?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc9e5e9-d7d3-4b32-a02a-187c1a921bbe",
   "metadata": {},
   "source": [
    "Bayes' theorem and conditional probability are closely related concepts in probability theory, and Bayes' theorem can be derived from conditional probability. Here's how they are connected:\r\n",
    "\r\n",
    "1. **Conditional Probability**: Conditional probability is the probability of one event occurring given that another event has already occurred. It is denoted as \\( P(A \\mid B) \\), which represents the probability of event \\( A \\) given that event \\( B \\) has occurred. The formula for conditional probability is:\r\n",
    "   \\[ P(A \\mid B) = \\frac{P(A \\cap B)}{P(B)} \\]\r\n",
    "   where \\( P(A \\cap B) \\) is the probability of both \\( A \\) and \\( B \\) occurring together, and \\( P(B) \\) is the probability of event \\( B \\) occurring.\r\n",
    "\r\n",
    "2. **Bayes' Theorem**: Bayes' theorem relates conditional probabilities in a specific way, allowing us to update our beliefs about the occurrence of one event based on the occurrence of another event. The theorem is stated as:\r\n",
    "   \\[ P(A \\mid B) = \\frac{P(B \\mid A) \\times P(A)}{P(B)} \\]\r\n",
    "   where:\r\n",
    "   - \\( P(A \\mid B) \\) is the posterior probability of \\( A \\) given \\( B \\),\r\n",
    "   - \\( P(B \\mid A) \\) is the likelihood of \\( B \\) given \\( A \\),\r\n",
    "   - \\( P(A) \\) is the prior probability of \\( A \\),\r\n",
    "   - \\( P(B) \\) is the total probability of \\( B \\).\r\n",
    "\r\n",
    "3. **Deriving Bayes' Theorem from Conditional Probability**: Bayes' theorem can be derived from the definition of conditional probability. By rearranging the definition of conditional probability, we have:\r\n",
    "   \\[ P(A \\cap B) = P(A \\mid B) \\times P(B) \\]\r\n",
    "   Similarly,\r\n",
    "   \\[ P(B \\cap A) = P(B \\mid A) \\times P(A) \\]\r\n",
    "   Now, by symmetry of intersection (i.e., \\( P(A \\cap B) = P(B \\cap A) \\)), we can equate these expressions:\r\n",
    "   \\[ P(A \\mid B) \\times P(B) = P(B \\mid A) \\times P(A) \\]\r\n",
    "   Rearranging this equation yields Bayes' theorem:\r\n",
    "   \\[ P(A \\mid B) = \\frac{P(B \\mid A) \\times P(A)}{P(B)} \\]\r\n",
    "\r\n",
    "Therefore, Bayes' theorem provides a systematic way to update probabilities based on new evidence (likelihood) and existing beliefs (prior probabilities), allowing us to compute the probability of an event conditioned on the occurrence of another event. This relationship with conditional probability is fundamental in Bayesian statistics and inference."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51a948f-a1e5-479d-94b3-88601844727f",
   "metadata": {},
   "source": [
    "#### Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eea53d-e8f5-4fb2-97af-efa4746cb9c8",
   "metadata": {},
   "source": [
    "Choosing the appropriate type of Naive Bayes classifier for a given problem depends on several factors including the nature of the data, assumptions about the data distribution, and the specific characteristics of the problem. Here are steps and considerations to guide the selection process:\r\n",
    "\r\n",
    "1. **Understand the Types of Naive Bayes Classifiers**:\r\n",
    "   There are different variants of Naive Bayes classifiers, including:\r\n",
    "   - **Gaussian Naive Bayes**: Assumes that continuous features follow a Gaussian (normal) distribution.\r\n",
    "   - **Multinomial Naive Bayes**: Suitable for features that represent counts or frequencies (e.g., word counts in text classification).\r\n",
    "   - **Bernoulli Naive Bayes**: Assumes features are binary (e.g., presence/absence of a feaaracteristics of your data.\r\n",
    "\r\n",
    "In summary, the choice of Naive Bayes classifier depends on the type of features in the dataset, the assumptions about feature distributions, and the specific requirements and characteristics of the problem at hand. Experimentation and evaluation are crucial for determining which variant performs best for a given task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af989599-dcd0-41fb-b945-64717fbecbba",
   "metadata": {},
   "source": [
    "#### Q6. Assignment:\n",
    "#### \n",
    "You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naiv \r\n",
    "Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency  f\r\n",
    "each feature value for each cla\n",
    "\r\n",
    "Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=\n",
    "\r\n",
    "A 3 3 4 4 3 3\n",
    "\n",
    "3\r\n",
    "B 2 2  2 2 3\n",
    "\n",
    "#### Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e5b2e0-7434-4b5e-82ef-a68eea56681b",
   "metadata": {},
   "source": [
    "Actually I don't understand the given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5088c230-933e-4fef-884a-f5348b208aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
